%% This is an example first chapter.  You should put chapter/appendix that you
%% write into a separate file, and add a line \include{yourfilename} to
%% main.tex, where `yourfilename.tex' is the name of the chapter/appendix file.
%% You can process specific files by typing their names in at the 
%% \files=
%% prompt when you run the file main.tex through LaTeX.
\chapter{Introduction}
\section{Motivation}
Data centers increasingly use server virtualization to
reduce operating costs, simplify administrative tasks and improve 
performance scalability. Through virtualization, it is
possible to achieve high resource utilization and isolation at the same time:
each application is typically assigned a dedicated server virtual machine (VM), 
while many VMs are consolidated on powerful host computers
to reduce wasted cycles. The use of techniques such as memory overcommitment (including 
\emph{transparent page sharing, ballooning} and \emph{hypervisor swapping}) \cite{waldspurger2002memory} 
has further improved the consolidation ratios and cost-effectiveness 
of server virtualization, and augurs well for the future of the technology.

Given the success of server virtualization, many companies are 
extending the use of virtualization to their desktop computers.
In a Virtual Desktop Infrastructure \cite{vmwarevdi} (VDI), 
desktop operating systems and applications are hosted in 
virtual machines that reside in a data center; 
users access virtual desktops from desktop PCs or thin clients
via a remote display protocol. A VDI provides simplicity 
in administration and management: applications
can be centrally added, deleted, upgraded and patched. 
Furthermore, desktop virtualization promises even
higher consolidation ratios than server virtualization
because desktop virtual machines typically require
less resources than server virtual machines.

Consolidation ratios (equivalently, VM density per host) in data centers
are expected to increase in the future, not only because of
improvements in virtualization technology, but also because
new generations of processors support more cores and 
more memory \cite{hansen2010lithium}. 
Because a single VM would typically utilize only a modest fraction of a 
host's hardware resources, a high VM density per host is
desireable in most cases for effective resource utilization. 
However, correlated spikes in the CPU/memory usage of many VMs can suddenly 
cripple host machines. For instance, a \emph{boot storm} \cite{hansen2010lithium, 
liao2011vmstore, meng2010tide, rajan2010vdc, vaghani2010virtual}
can occur after some software is installed or updated, requiring hundreds 
or thousands of identical VMs to reboot at the same time.
Bootstorms can be particularly frequent in VDIs because 
users typically show up to work at roughly the same time
in the morning each day. 

Concurrently booting VMs create unusually high I/O traffic,
generate numerous disk and memory allocation requests,
and can saturate host CPUs. 
To avoid the prohibitively high boot latencies that  
result from boot storms, data centers usually either 
boot machines in a staggered fashion, or invest in specialized,
expensive and/or extra-provisioned hardware for network/storage \cite{highperfnas, liao2011vmstore}.
There is also anecdotal evidence that VDI users sometimes leave their desktop computers running
overnight to avoid morning boot storms; this practice
represents an unnecessary addition to already exorbitant
data center energy bills \cite{qureshi2009bills}. Data deduplication \cite{clements2009deduplication},
through which hosts reclaim/reuse disk pages common to several VMs, 
has been proven to reduce the memory footprint of concurrently
booting machines. However, while data deduplication can mitigate the stress on the memory subsystem in
a boot storm, lowered memory latency can in turn overwhelm the CPU, 
fibre channel, bus infrastructure or controller resources 
and simply turn them into bottlenecks instead \cite{netappstorm}. 

With the spread of virtualization, it is important to address the
bootstorm problem in a way that does not involve simply skirting around the
issue. Data deduplication is partly effective because identical VMs load
the same data from disk when they boot up. In this thesis, we pose the following question: 
is it possible to generalize deduplication of data to deduplication of \emph{execution}?
If many identical VMs are concurrently booting up in a data center, 
do they execute the same set of instructions? Even if there are
some differences in the instructions executed, are they caused by
controllable sources of non-determinism? Ultimately, if there is a way
to ensure that concurrently booting VMs execute mostly the same set of instructions
and perform the same I/O requests, one way to solve the boot storm
problem may be remarkable simple in essence: instead of booting
$N$ identical VMs concurrently, we can boot one VM as a leader; the remaining $(N-1)$ VMs minimally
follow the leader by executing a tiny subset of the instructions they would otherwise execute;
we fork execution into $N$ different instances as late as possible into the boot process. This approach
could potentially reduce pressure on the underlying host hardware,
and thereby enable data centers to handle boot storms effectively.

\section{Goal of Thesis}
This thesis aims to address the following questions:

\begin{enumerate}

\item When identical VMs boot up concurrently, how similar
are the sets of instructions executed? What is the statistical
profile of any differences in the distinct instruction streams?

\item What are the source(s) of any differences in
the instruction streams of concurrently booting VMs?
Are there ways to minimize the non-determinism in
booting VMs?

\end{enumerate}

The answers to these questions clearly are crucial in determining
the feasibility of \emph{deduplication of execution} as a possible solution
to the boot storm problem. 

\section{Contrbutions}
For this work, we used dynamic instrumentation frameworks such as Pin \cite{luk2005pin} and
DynamoRio \cite{bruening2004dr} to study user-level instruction streams from a
a few representative Linux services at boot-time. 
Specifically, we:

\begin {enumerate}
\item show that nondeterminism in Linux services (such as \texttt{cron}, \texttt{cups} and \texttt{ntp}) 
  is bursty and extremely rare;
\item document the sources of non-determinism in Linux services -- both obvious and obscure --
  and specify strategies for overcoming them in the boot storm scenario;
\item use simple Dynamic Instrumentation techniques to show that \emph{fully} deterministic execution is achievable
  without \emph{any} modifications to Linux or an executing service.
\end {enumerate}

Strategies to achieve deterministic execution have been studied at the operating system layer \cite{bergan2010dos} before,
but they require modifications to Linux. Deterministic execution can be achieved in multi-threaded programs 
using record-and-replay approaches \cite{marek2011scaling, patil2010pinplay}. Our study of non-determinism has somewhat different goals from 
from both approaches: we wish to avoid changing existing software to ease adoption and make 
several distinct -- and potentially semantically different -- executions \emph{overlap} as much as possible, 
rather than replay one execution over and over. In our case, we do not know \emph{a priori} whether two executions 
will behave similarly or not. That the behavior of system calls or signals in Linux can lead to different results or side-effects across
multiple executions of an application is well known: what is not documented is the application \emph{context} in
which these sources of non-determinism originate. To the best of our knowledge,this is
the first attempt to study the statistical profile of non-determinism in Linux services.

Finally, while we hope that this work ultimately provides the basis for an implementation of our
proposed solution, we note that deterministic execution can immediately improve
the effectiveness of existing technologies such as transparent page sharing
and data deduplication.

\section{Importance of Deterministic Execution}
While our study of nondeterminism is driven by a specfic application,
deterministic execution of programs can be beneficial in many
different scenarios in its own right. Our work complements existing
work on deterministc execution because 
it focuses on deterministic execution of primarily single-threaded services in Linux,
at the granularity of individual instructions and their side-effects in
memory. The motivations for deterministic multhreading listed in
\cite{marek2011scaling, patil2010pinplay} apply to our work as well.

{\bf Mainstream Computing and Security} \newline
If repeated executions of the same program can be
expected to execute the same set of instructions, then
any deviations can be used to detect security
attacks on applications. Detection of such
anomalous executions is the focus of \emph{mainstream computing} \cite{stephenson2010mainstream},
and deterministic execution obviously helps in reducing false alarms.

{\bf Testing} \newline
Deterministic execution in general facilitates testing,
because outputs and internal state can be checked at 
certain points with respect to expected values. Our version
of determinism allows for a particularly strong kind
of test case that may be necessary for safety-critical 
systems: with deterministic execution, a program 
must execute the exact same instructions 
across different executions, for the same inputs.
Test cases can check for deviations from 
the expected \emph{instruction} sequences, and not
just the expected outputs (which may mask bugs).

{\bf Debugging} \newline
Erroneous behavior can be more easily reproduced
via determininstic execution, which
helps with debugging. Deterministic
execution has much lower storage overhead
than traditional record-and-replay approaches. 

\section{Thesis Organization}
In what follows, Chapter 2 presents an overview of
how we used dynamic instrumentation to measure
and profile non-determinism in Linux services.
Chapter 3 presents the case study of a few selected Linux services
in detail to list the sources of non-determinism we
found and how we overcame them. 
Chapter 4 presents design ideas for an implemenation of deduplication of execution.
Chapter 5 summarizes related work. Finally, Chapter 6 concludes this thesis
and discusses future work.


% This is an example of how you would use tgrind to include an example
% of source code; it is commented out in this template since the code
% example file does not exist.  To use it, you need to remove the '%' on the
% beginning of the line, and insert your own information in the call.
%
%\tagrind[htbp]{code/pmn.s.tex}{Post Multiply Normalization}{opt:pmn}

% This is an example of how you would use tgrind to include an example
% of source code; it is commented out in this template since the code
% example file does not exist.  To use it, you need to remove the '%' on the
% beginning of the line, and insert your own information in the call.
%
%\tgrind[htbp]{code/be.s.tex}{Block Exponent}{opt:be}

